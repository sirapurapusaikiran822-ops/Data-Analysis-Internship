{"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.impute import SimpleImputer\n","\n","# Initialize df to None, so it's always defined\n","df = None\n","\n","# 1. Load the Dataset\n","try:\n","    df = pd.read_csv('dataset.csv')\n","    print(\"‚úÖ Dataset loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"‚ùå dataset.csv not found. Please ensure the file is in the same directory.\")\n","    # Removed exit() as it can cause NameError in Colab if subsequent code runs\n","    # Further processing will be skipped due to df being None\n","\n","# Only proceed if the dataset was loaded successfully\n","if df is not None:\n","    # --- 2. Exploratory Data Analysis & Cleaning ---\n","\n","    print(f\"\\nOriginal Dataset Shape: {df.shape}\")\n","\n","    # Drop irrelevant columns for prediction (IDs, Dates, and Text Reviews)\n","    # We drop 'Date' because it requires complex time-series feature engineering\n","    # which is outside the scope of a basic model.\n","    cols_to_drop = ['OrderID', 'CustomerID', 'Date', 'Review']\n","    df = df.drop(columns=cols_to_drop, errors='ignore')\n","\n","    # Handling Missing Values\n","    # - Numerical columns (Quantity, UnitPrice, Rating): Fill with the Median\n","    num_cols = ['Quantity', 'UnitPrice', 'TotalAmount', 'Rating']\n","    num_imputer = SimpleImputer(strategy='median')\n","    df[num_cols] = num_imputer.fit_transform(df[num_cols])\n","\n","    # - Categorical columns (Country, etc.): Fill with the \"Most Frequent\" value (Mode)\n","    cat_cols = ['ProductCategory', 'PaymentMethod', 'Country']\n","    cat_imputer = SimpleImputer(strategy='most_frequent')\n","    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n","\n","    # Drop rows where the Target (DeliveryStatus) is missing, if any\n","    df = df.dropna(subset=['DeliveryStatus'])\n","\n","    print(\"‚úÖ Data cleaning complete.\")\n","\n","    # --- 3. Feature Engineering ---\n","\n","    # Encode Categorical Variables (Convert text to numbers)\n","    # We use LabelEncoder for the target and GetDummies (One-Hot) for features\n","    le = LabelEncoder()\n","    df['DeliveryStatus'] = le.fit_transform(df['DeliveryStatus'])\n","\n","    print(f\"Target Classes: {dict(zip(le.transform(le.classes_), le.classes_))}\")\n","\n","    # One-Hot Encode the remaining categorical features\n","    df_encoded = pd.get_dummies(df, columns=['ProductCategory', 'PaymentMethod', 'Country'], drop_first=True)\n","\n","    # --- 4. Splitting the Data ---\n","\n","    X = df_encoded.drop('DeliveryStatus', axis=1) # Features\n","    y = df_encoded['DeliveryStatus']              # Target\n","\n","    # Split into Training (80%) and Testing (20%) sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # --- 5. Model Training ---\n","\n","    print(\"\\nüöÄ Training Random Forest Classifier...\")\n","    model = RandomForestClassifier(n_estimators=100, random_state=42)\n","    model.fit(X_train, y_train)\n","\n","    # --- 6. Evaluation ---\n","\n","    y_pred = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    print(f\"\\nüèÜ Model Accuracy: {accuracy:.2%}\")\n","    print(\"\\n--- Classification Report ---\")\n","    print(classification_report(y_test, y_pred, target_names=le.classes_))\n","\n","    # --- 7. Feature Importance Analysis ---\n","    # Let's see what factors actually drive the Delivery Status\n","    feature_importances = pd.DataFrame({\n","        'Feature': X.columns,\n","        'Importance': model.feature_importances_\n","    }).sort_values(by='Importance', ascending=False)\n","\n","    print(\"\\n--- Top 5 Factors Influencing Delivery Status ---\")\n","    print(feature_importances.head(5))\n","else:\n","    print(\"\\nSkipping further processing as the dataset was not loaded.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["‚ùå dataset.csv not found. Please ensure the file is in the same directory.\n","\n","Skipping further processing as the dataset was not loaded.\n"]}],"execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5Iu6P0K22LH","executionInfo":{"status":"ok","timestamp":1763534851538,"user_tz":-330,"elapsed":1644,"user":{"displayName":"Sirapurapu Saikiran","userId":"18422185545090887757"}},"outputId":"0b0c316f-ccaf-4a0a-a6ee-dbfb20525183"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1f7419f","executionInfo":{"status":"ok","timestamp":1763534966298,"user_tz":-330,"elapsed":378,"user":{"displayName":"Sirapurapu Saikiran","userId":"18422185545090887757"}},"outputId":"a3ff70a2-77b8-4631-da29-81b023c8138b"},"source":["import os\n","\n","if 'dataset.csv' in os.listdir('.'):\n","    print(\"‚úÖ 'dataset.csv' found in the current directory.\")\n","else:\n","    print(\"‚ùå 'dataset.csv' not found. Please ensure it has been uploaded.\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ 'dataset.csv' found in the current directory.\n"]}]},{"cell_type":"markdown","metadata":{"id":"a7d6d3ba"},"source":["# Task\n","Execute the code cell `h5Iu6P0K22LH` to prepare the data, train the model, and evaluate its performance, including feature importance analysis."]},{"cell_type":"markdown","metadata":{"id":"32bc997c"},"source":["## Prepare Data and Train Model\n","\n","### Subtask:\n","Execute the existing code cell to load the dataset, perform data cleaning and feature engineering, split the data, train a RandomForestClassifier, and evaluate its performance. This step will also perform feature importance analysis.\n"]},{"cell_type":"markdown","metadata":{"id":"ba76ab00"},"source":["**Reasoning**:\n","The subtask requires executing an existing code cell to perform data loading, cleaning, feature engineering, model training, and evaluation. I will provide the code from the specified cell.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17d5ed6b","executionInfo":{"status":"ok","timestamp":1763535846327,"user_tz":-330,"elapsed":489,"user":{"displayName":"Sirapurapu Saikiran","userId":"18422185545090887757"}},"outputId":"4e2b264e-88b7-4637-ff6f-31979979a6c8"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.impute import SimpleImputer\n","\n","# Initialize df to None, so it's always defined\n","df = None\n","\n","# 1. Load the Dataset\n","try:\n","    df = pd.read_csv('dataset.csv')\n","    print(\"‚úÖ Dataset loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"‚ùå dataset.csv not found. Please ensure the file is in the same directory.\")\n","    # Removed exit() as it can cause NameError in Colab if subsequent code runs\n","    # Further processing will be skipped due to df being None\n","\n","# Only proceed if the dataset was loaded successfully\n","if df is not None:\n","    # --- 2. Exploratory Data Analysis & Cleaning ---\n","\n","    print(f\"\\nOriginal Dataset Shape: {df.shape}\")\n","\n","    # Drop irrelevant columns for prediction (IDs, Dates, and Text Reviews)\n","    # We drop 'Date' because it requires complex time-series feature engineering\n","    # which is outside the scope of a basic model.\n","    cols_to_drop = ['OrderID', 'CustomerID', 'Date', 'Review']\n","    df = df.drop(columns=cols_to_drop, errors='ignore')\n","\n","    # Handling Missing Values\n","    # - Numerical columns (Quantity, UnitPrice, Rating): Fill with the Median\n","    num_cols = ['Quantity', 'UnitPrice', 'TotalAmount', 'Rating']\n","    num_imputer = SimpleImputer(strategy='median')\n","    df[num_cols] = num_imputer.fit_transform(df[num_cols])\n","\n","    # - Categorical columns (Country, etc.): Fill with the \"Most Frequent\" value (Mode)\n","    cat_cols = ['ProductCategory', 'PaymentMethod', 'Country']\n","    cat_imputer = SimpleImputer(strategy='most_frequent')\n","    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n","\n","    # Drop rows where the Target (DeliveryStatus) is missing, if any\n","    df = df.dropna(subset=['DeliveryStatus'])\n","\n","    print(\"‚úÖ Data cleaning complete.\")\n","\n","    # --- 3. Feature Engineering ---\n","\n","    # Encode Categorical Variables (Convert text to numbers)\n","    # We use LabelEncoder for the target and GetDummies (One-Hot) for features\n","    le = LabelEncoder()\n","    df['DeliveryStatus'] = le.fit_transform(df['DeliveryStatus'])\n","\n","    print(f\"Target Classes: {dict(zip(le.transform(le.classes_), le.classes_))}\")\n","\n","    # One-Hot Encode the remaining categorical features\n","    df_encoded = pd.get_dummies(df, columns=['ProductCategory', 'PaymentMethod', 'Country'], drop_first=True)\n","\n","    # --- 4. Splitting the Data ---\n","\n","    X = df_encoded.drop('DeliveryStatus', axis=1) # Features\n","    y = df_encoded['DeliveryStatus']              # Target\n","\n","    # Split into Training (80%) and Testing (20%) sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # --- 5. Model Training ---\n","\n","    print(\"\\nüöÄ Training Random Forest Classifier...\")\n","    model = RandomForestClassifier(n_estimators=100, random_state=42)\n","    model.fit(X_train, y_train)\n","\n","    # --- 6. Evaluation ---\n","\n","    y_pred = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    print(f\"\\nüèÜ Model Accuracy: {accuracy:.2%}\")\n","    print(\"\\n--- Classification Report ---\")\n","    print(classification_report(y_test, y_pred, target_names=le.classes_))\n","\n","    # --- 7. Feature Importance Analysis ---\n","    # Let's see what factors actually drive the Delivery Status\n","    feature_importances = pd.DataFrame({\n","        'Feature': X.columns,\n","        'Importance': model.feature_importances_\n","    }).sort_values(by='Importance', ascending=False)\n","\n","    print(\"\\n--- Top 5 Factors Influencing Delivery Status ---\")\n","    print(feature_importances.head(5))\n","else:\n","    print(\"\\nSkipping further processing as the dataset was not loaded.\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Dataset loaded successfully.\n","\n","Original Dataset Shape: (600, 12)\n","‚úÖ Data cleaning complete.\n","Target Classes: {np.int64(0): 'Delivered', np.int64(1): 'In Transit', np.int64(2): 'Returned'}\n","\n","üöÄ Training Random Forest Classifier...\n","\n","üèÜ Model Accuracy: 76.32%\n","\n","--- Classification Report ---\n","              precision    recall  f1-score   support\n","\n","   Delivered       0.77      0.71      0.74        34\n","  In Transit       0.74      0.88      0.81        33\n","    Returned       0.77      0.72      0.75        47\n","\n","    accuracy                           0.76       114\n","   macro avg       0.76      0.77      0.76       114\n","weighted avg       0.76      0.76      0.76       114\n","\n","\n","--- Top 5 Factors Influencing Delivery Status ---\n","                    Feature  Importance\n","2               TotalAmount    0.217411\n","1                 UnitPrice    0.213598\n","0                  Quantity    0.102075\n","3                    Rating    0.099011\n","9  PaymentMethod_Debit Card    0.036278\n"]}]},{"cell_type":"markdown","metadata":{"id":"cdb133cd"},"source":["## Explain Model Evaluation Results\n","\n","### Subtask:\n","Analyze and explain the accuracy, classification report, and feature importance generated by the model.\n"]},{"cell_type":"markdown","metadata":{"id":"97d6a048"},"source":["### Model Evaluation Results Analysis\n","\n","1.  **Overall Model Accuracy**: The model achieved an accuracy of **76.32%**. This means that approximately 76.32% of the time, the model correctly predicted the `DeliveryStatus` (Delivered, In Transit, or Returned) on the unseen test data. While 76% is a reasonable starting point, it also indicates that there's room for improvement, as nearly a quarter of predictions were incorrect.\n","\n","2.  **Classification Report Analysis**:\n","    *   **Precision**: Precision measures the accuracy of positive predictions. For example, a precision of 0.77 for 'Delivered' means that when the model predicts a delivery as 'Delivered', it is correct 77% of the time.\n","    *   **Recall**: Recall (or sensitivity) measures the proportion of actual positives that were identified correctly. A recall of 0.71 for 'Delivered' means that the model correctly identified 71% of all actual 'Delivered' statuses.\n","    *   **F1-score**: The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both. It's particularly useful when dealing with imbalanced classes. All classes show a decent F1-score, ranging from 0.74 to 0.81.\n","\n","    **Class-wise Observations**:\n","    *   **Delivered**: The model has a precision of 0.77 and a recall of 0.71. This suggests that while the model is fairly accurate when it predicts 'Delivered', it misses some instances of actual 'Delivered' items.\n","    *   **In Transit**: This class has the highest recall (0.88) and F1-score (0.81), indicating the model is very good at identifying actual 'In Transit' items and has a good balance between precision and recall for this class.\n","    *   **Returned**: Similar to 'Delivered', the 'Returned' class has a precision of 0.77 and a recall of 0.72. The model performs comparably well for 'Returned' items as it does for 'Delivered' items.\n","\n","    The `support` column indicates the number of actual occurrences of each class in the test set. The `macro avg` and `weighted avg` provide overall averages of these metrics across all classes.\n","\n","3.  **Top 5 Factors Influencing Delivery Status**:\n","    The feature importance analysis helps understand which input variables contribute most to the model's prediction of `DeliveryStatus`.\n","    *   **TotalAmount (0.217)**: This is the most significant factor. It suggests that the total value of an order strongly influences its delivery status. Higher total amounts might correlate with more attention during shipping, different shipping methods, or higher likelihood of being 'Returned' due to customer expectation or fraud.\n","    *   **UnitPrice (0.214)**: Similar to `TotalAmount`, the price of a single unit also plays a crucial role. This could be related to the nature of high-value items requiring special handling.\n","    *   **Quantity (0.102)**: The number of items in an order is also important. Larger quantities might lead to more complex logistics, higher chances of partial delivery, or increased potential for issues.\n","    *   **Rating (0.099)**: The product rating (likely customer review rating) is a notable factor. Higher or lower ratings could indirectly correlate with product popularity, customer satisfaction, and thus impact delivery or return rates.\n","    *   **PaymentMethod_Debit Card (0.036)**: This indicates that using a Debit Card as a payment method has a specific, albeit smaller, influence on delivery status compared to other payment methods. This could imply differences in fraud rates, transaction processing, or customer demographics associated with this payment method.\n","\n","    These factors collectively suggest that the financial aspects of an order (`TotalAmount`, `UnitPrice`, `Quantity`) and initial customer perception (`Rating`) are the primary drivers for predicting delivery outcomes."]},{"cell_type":"markdown","metadata":{"id":"ef60fea8"},"source":["## Final Task\n","\n","### Subtask:\n","Summarize the model building process and its key findings, including the performance metrics and important features.\n"]},{"cell_type":"markdown","metadata":{"id":"4feba76a"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","*   The dataset was successfully loaded, cleaned by dropping irrelevant columns (`OrderID`, `CustomerID`, `Date`, `Review`), and handling missing values. Numerical features (`Quantity`, `UnitPrice`, `TotalAmount`, `Rating`) were imputed with their median, while categorical features (`ProductCategory`, `PaymentMethod`, `Country`) used the mode.\n","*   The target variable `DeliveryStatus` was label encoded, and other categorical features were one-hot encoded for model training.\n","*   A `RandomForestClassifier` was trained and achieved an overall accuracy of **76.32%** on the test set.\n","*   The classification report revealed that the model performed best for the 'In Transit' class, with a recall of **0.88** and an F1-score of **0.81**. 'Delivered' and 'Returned' classes had comparable performance, with F1-scores of 0.74 and 0.75, respectively.\n","*   Feature importance analysis identified the top 5 factors influencing `DeliveryStatus`: `TotalAmount` (importance: 0.217), `UnitPrice` (importance: 0.214), `Quantity` (importance: 0.102), `Rating` (importance: 0.099), and `PaymentMethod_Debit Card` (importance: 0.036). This highlights that financial aspects of an order are the primary drivers.\n","\n","### Insights or Next Steps\n","\n","*   Explore advanced feature engineering, such as creating interaction terms between financial features (`TotalAmount`, `UnitPrice`, `Quantity`), or incorporating external data, to potentially improve model performance, especially for the 'Delivered' and 'Returned' classes.\n","*   Given the high importance of `TotalAmount`, `UnitPrice`, and `Quantity`, businesses could investigate how order value and item quantity influence logistics and delivery outcomes to identify specific bottlenecks or optimize shipping strategies for different order profiles.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}